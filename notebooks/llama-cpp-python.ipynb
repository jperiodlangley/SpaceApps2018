{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jperiodlangley/SpaceApps2018/blob/master/notebooks/llama-cpp-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YkjlW4gZkdbJ",
        "collapsed": true,
        "outputId": "f65506df-9818-49d3-bfed-1d9f785c72af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n",
            "Collecting llama-cpp-python==0.2.7\n",
            "  Downloading https://github.com/jllllll/llama-cpp-python-cuBLAS-wheels/releases/download/wheels/llama_cpp_python-0.2.7%2Bcu122-cp310-cp310-manylinux_2_31_x86_64.whl (14.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.8/14.8 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.7) (4.12.2)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.10/dist-packages (from llama-cpp-python==0.2.7) (1.25.2)\n",
            "Collecting diskcache>=5.6.1 (from llama-cpp-python==0.2.7)\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
            "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n",
            "Successfully installed diskcache-5.6.3 llama-cpp-python-0.2.7+cu122\n"
          ]
        }
      ],
      "source": [
        "#Install llama-cpp-python, cuda-enabled package\n",
        "!python -m pip install llama-cpp-python==0.2.7 --prefer-binary --extra-index-url=https://jllllll.github.io/llama-cpp-python-cuBLAS-wheels/AVX2/cu122\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-13B-GGUF/resolve/main/llama-2-13b.Q5_K_M.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "SdO80Oi8HNuL",
        "outputId": "03dcc73c-a797-42d0-c7da-a23440ea8336"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-27 21:36:36--  https://huggingface.co/TheBloke/Llama-2-13B-GGUF/resolve/main/llama-2-13b.Q5_K_M.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.17, 18.164.174.118, 18.164.174.23, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.17|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/b1/be/b1bebadea37cd4dd7655da740ed74e9a825af7c9ffddf51690f59fb89df02706/c81202d8fd069ebcea9d44b2c2ff99a57f0accc3d73828ffdfcc538428aa5623?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b.Q5_K_M.gguf%22%3B&Expires=1722375397&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjM3NTM5N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMS9iZS9iMWJlYmFkZWEzN2NkNGRkNzY1NWRhNzQwZWQ3NGU5YTgyNWFmN2M5ZmZkZGY1MTY5MGY1OWZiODlkZjAyNzA2L2M4MTIwMmQ4ZmQwNjllYmNlYTlkNDRiMmMyZmY5OWE1N2YwYWNjYzNkNzM4MjhmZmRmY2M1Mzg0MjhhYTU2MjM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jh44Vi72f7e0iHM1U1158f5rIFuJovZGap6v1NVBlNWUmAtOoqhHZ4ItwPek8Qva97F%7EunGunmPGNBOOw--Yx9iHOqfMFw28wxzV%7EMxiKHFOdB2aDV8Ox%7ECRV0XF6qoW0B4FzmTmvC-fv0WAPQB5If0TFJTn16Jn8wcQddF5bPalp7ARG5paCBnVIGk%7EAeI9FHiYxZxFl0JUF3w4PwlwsHmkE4eKSGqlc5qH6rdZCvgM7IIAvj1bsR5Rvpb1qz4HAR9x12h9MHD7l1wFb1uw3vZZvLlN9qGEBcOKvTO%7EnpkwyCie8ix47c6796EDuUaI4-AcnyEiP3bTfKruDf8YZQ__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-07-27 21:36:37--  https://cdn-lfs.huggingface.co/repos/b1/be/b1bebadea37cd4dd7655da740ed74e9a825af7c9ffddf51690f59fb89df02706/c81202d8fd069ebcea9d44b2c2ff99a57f0accc3d73828ffdfcc538428aa5623?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b.Q5_K_M.gguf%22%3B&Expires=1722375397&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjM3NTM5N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy9iMS9iZS9iMWJlYmFkZWEzN2NkNGRkNzY1NWRhNzQwZWQ3NGU5YTgyNWFmN2M5ZmZkZGY1MTY5MGY1OWZiODlkZjAyNzA2L2M4MTIwMmQ4ZmQwNjllYmNlYTlkNDRiMmMyZmY5OWE1N2YwYWNjYzNkNzM4MjhmZmRmY2M1Mzg0MjhhYTU2MjM%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jh44Vi72f7e0iHM1U1158f5rIFuJovZGap6v1NVBlNWUmAtOoqhHZ4ItwPek8Qva97F%7EunGunmPGNBOOw--Yx9iHOqfMFw28wxzV%7EMxiKHFOdB2aDV8Ox%7ECRV0XF6qoW0B4FzmTmvC-fv0WAPQB5If0TFJTn16Jn8wcQddF5bPalp7ARG5paCBnVIGk%7EAeI9FHiYxZxFl0JUF3w4PwlwsHmkE4eKSGqlc5qH6rdZCvgM7IIAvj1bsR5Rvpb1qz4HAR9x12h9MHD7l1wFb1uw3vZZvLlN9qGEBcOKvTO%7EnpkwyCie8ix47c6796EDuUaI4-AcnyEiP3bTfKruDf8YZQ__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.163.125.41, 3.163.125.111, 3.163.125.12, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.163.125.41|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9229924224 (8.6G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b.Q5_K_M.gguf?download=true’\n",
            "\n",
            "llama-2-13b.Q5_K_M. 100%[===================>]   8.60G  56.6MB/s    in 3m 15s  \n",
            "\n",
            "2024-07-27 21:39:52 (45.2 MB/s) - ‘llama-2-13b.Q5_K_M.gguf?download=true’ saved [9229924224/9229924224]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mv llama-2-13b.Q5_K_M.gguf?download=true llama-2-13b.Q5_K_M.gguf"
      ],
      "metadata": {
        "id": "7iVCeTBzI4Za"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "havBbA5-LeUg",
        "outputId": "222d119c-4195-4e4e-c2b2-204f1ba1c233"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-27 21:52:32--  https://huggingface.co/TheBloke/Llama-2-13B-chat-GGUF/resolve/main/llama-2-13b-chat.Q5_K_M.gguf?download=true\n",
            "Resolving huggingface.co (huggingface.co)... 18.164.174.118, 18.164.174.23, 18.164.174.17, ...\n",
            "Connecting to huggingface.co (huggingface.co)|18.164.174.118|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1722376352&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjM3NjM1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jB6ONoU2CpJC4FTq-0fPfAVkMkJRuhPeMGSk9nIuDMaH77ICk%7EXQgwUeR8cosOM9hj8uf0DJnnq00ng68HtvNYbXl1M8D5KAPK7R9T3RQKKouISegX3UJjqnWJYkybmIxumSBao7Hc-%7E%7EB9CcFxoVrRze3NqZp8dF5HF3eZkwK1ndewe8Actlmt-eNvE-%7EtF2vKN17FrtuPLeJbqvgJ5k8%7ETmf6DHLStx7N4h8ViFuABi9H57NSthNuZyRC2KO4oL4tyYyjCSkNGycGo6wkVgxWgNUjgeqtYnKmfxZrvp1JVCQroBR4qEXce0FH5bW-vWAeyeXRUhCmXsUSGQe2l-Q__&Key-Pair-Id=K3ESJI6DHPFC7 [following]\n",
            "--2024-07-27 21:52:32--  https://cdn-lfs.huggingface.co/repos/8d/b1/8db1d1f73b4caa58e947ccbfe2fb27ac5e495c2ad8457ad299d15987aee3b520/ef36e090240040f97325758c1ad8e23f3801466a8eece3a9eac2d22d942f548a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27llama-2-13b-chat.Q5_K_M.gguf%3B+filename%3D%22llama-2-13b-chat.Q5_K_M.gguf%22%3B&Expires=1722376352&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyMjM3NjM1Mn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy84ZC9iMS84ZGIxZDFmNzNiNGNhYTU4ZTk0N2NjYmZlMmZiMjdhYzVlNDk1YzJhZDg0NTdhZDI5OWQxNTk4N2FlZTNiNTIwL2VmMzZlMDkwMjQwMDQwZjk3MzI1NzU4YzFhZDhlMjNmMzgwMTQ2NmE4ZWVjZTNhOWVhYzJkMjJkOTQyZjU0OGE%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=jB6ONoU2CpJC4FTq-0fPfAVkMkJRuhPeMGSk9nIuDMaH77ICk%7EXQgwUeR8cosOM9hj8uf0DJnnq00ng68HtvNYbXl1M8D5KAPK7R9T3RQKKouISegX3UJjqnWJYkybmIxumSBao7Hc-%7E%7EB9CcFxoVrRze3NqZp8dF5HF3eZkwK1ndewe8Actlmt-eNvE-%7EtF2vKN17FrtuPLeJbqvgJ5k8%7ETmf6DHLStx7N4h8ViFuABi9H57NSthNuZyRC2KO4oL4tyYyjCSkNGycGo6wkVgxWgNUjgeqtYnKmfxZrvp1JVCQroBR4qEXce0FH5bW-vWAeyeXRUhCmXsUSGQe2l-Q__&Key-Pair-Id=K3ESJI6DHPFC7\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 3.163.125.12, 3.163.125.111, 3.163.125.79, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|3.163.125.12|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 9229924224 (8.6G) [binary/octet-stream]\n",
            "Saving to: ‘llama-2-13b-chat.Q5_K_M.gguf?download=true’\n",
            "\n",
            "5_K_M.gguf?download  92%[=================>  ]   7.99G  31.8MB/s    eta 15s    "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_cpp import Llama\n",
        "\n",
        "LLAMA_2_13B = \"llama-2-13b.Q5_K_M.gguf\"\n",
        "\n",
        "\n",
        "# Using the defaults here except for moving some of the layers to the GPU availalbe on this laptop.\n",
        "llm = Llama(model_path=LLAMA_2_13B, n_gpu_layers=-1,)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eNHjyAVXJCL1",
        "outputId": "a7ddde64-eeff-466b-f5f5-af9a45e0586e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This is a basic prompt to show building the context of the RAG and asking the question\n",
        "PROMPT = \"\"\"[INST] <<SYS>>You are a helpful, respectful and honest assistant. Always answer\n",
        "as helpfully as possible, while being safe. Your answers should not include any harmful,\n",
        "unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your\n",
        "responses are socially unbiased and positive in nature.\n",
        "\n",
        "If a question does not make any sense, or is not factually coherent, explain why instead of\n",
        "answering something not correct. If you don't know the answer to a question, please don't share\n",
        "false information.<</SYS>>\n",
        "\n",
        "Generate the next agent response by answering the question. Answer it as succinctly as possible.\n",
        "You are provided several documents with titles. If the answer comes from different documents\n",
        "please mention all possibilities in your answer and use the titles to separate between topics\n",
        "or domains. If you cannot answer the question from the given documents, please state that you\n",
        "do not have an answer.\n",
        "\n",
        "CONTEXT:\n",
        "\n",
        "Blog Post page 1: In this part, we will wrap the Transformer model with HuggingFace pipeline so that we can pass\n",
        "the rules to the Transformer model. To craft and pass the rules to the Transformer model,\n",
        "we can use the LangChain Prompt Template. In this prompt template, we can tell how the\n",
        "LLM should behave. This is shown in the pre_prompt variable. Next, we give some information\n",
        "or context to the LLM to refine our prediction. For example, we can tell the LLM that we are\n",
        "dicussing Apple product such as iphone, ipad and mac book, instead of discussing Apple as a\n",
        "fruit. With the context, we can pass in the relevant question as shown in the prompt variable.\n",
        "\n",
        "Question:\n",
        "\n",
        "What type of model do we pass the rules to?\n",
        "[/INST]\"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "wAL4pAKpJmxB"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Since the default of llama-cpp-python uses a 512 token context length, we need\n",
        "# to check and see how much our prompt uses.\n",
        "input_length = len(llm.tokenize(bytes(PROMPT, \"utf-8\")))\n",
        "\n",
        "print(f'Context length of prompt: {input_length}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olKxGjAlJt6g",
        "outputId": "fb449e1c-68cc-4d17-a2dc-8af8ccc728f2"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Context length of prompt: 422\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# To generate the answer, simply call the llm\n",
        "output = llm(\n",
        "    PROMPT, # Prompt\n",
        "    max_tokens=-1 # Generate tokens until we run out\n",
        ")\n",
        "\n",
        "# The output structure has a lot of additional stuff in it:\n",
        "print('\\nTotal output structure:')\n",
        "print(output)\n",
        "\n",
        "print('\\nOutput answer:')\n",
        "print(output['choices'][0]['text'])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tS-KvgEqJ8fQ",
        "outputId": "91e4c734-8859-4d7d-c287-ce03c5ee7540"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total output structure:\n",
            "{'id': 'cmpl-950fa4bd-2b4f-409b-8711-0295411d8826', 'object': 'text_completion', 'created': 1722116765, 'model': 'llama-2-13b.Q5_K_M.gguf', 'choices': [{'text': '\\n\\n<Answer>\\nI am not sure what you mean by \"pass the rules to\". Could you clarify what you are asking?</Answer>\\n', 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 422, 'completion_tokens': 32, 'total_tokens': 454}}\n",
            "\n",
            "Output answer:\n",
            "\n",
            "\n",
            "<Answer>\n",
            "I am not sure what you mean by \"pass the rules to\". Could you clarify what you are asking?</Answer>\n",
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyNfosrFrgiXWavdG3WCCRBl",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}